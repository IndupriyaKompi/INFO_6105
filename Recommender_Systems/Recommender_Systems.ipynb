{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "**What are recommender systems?**\n",
    "\n",
    "Recommender systems:  \n",
    "\n",
    "* Predict the \"rating\"   \n",
    "* Predict a list of items   \n",
    "\n",
    "\n",
    "A recommender system or a recommendation system is an information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.   \n",
    "\n",
    "Recommender systems do two things:   \n",
    "\n",
    "1) Predict the \"rating\"  a user would give to an item.     \n",
    "2) Predict a list of items a user would like.   \n",
    "\n",
    "Recommender systems are utilized in a variety of areas including movies, music, news, and nearly every website on the Internet.  \n",
    "\n",
    "See [https://en.wikipedia.org/wiki/Recommender_system](https://en.wikipedia.org/wiki/Recommender_system)   \n",
    "\n",
    "\n",
    "**Many types of recommender systems**\n",
    "\n",
    "\n",
    "There are many types of recommender systems: \n",
    "* Popularity based    \n",
    "* Content based     \n",
    "* Collaborative filtering        \n",
    "* Latent factor models    \n",
    "* more ....   \n",
    "\n",
    "All have strengths and weaknesses.  In the real-world most recommendation systems used a combination of approaches.   \n",
    "\n",
    "**User-Item Ratings Matrix**\n",
    "\n",
    "The primary data structure for recommender systems is the user-item ratings matrix.   \n",
    "\n",
    "* Rows are users   \n",
    "* Columns are items    \n",
    "* Each each contains a null value or rating  \n",
    "\n",
    "These are usually sparse matrices as there are typically many thousands of items and almost no users will rate more than a few items.  \n",
    "\n",
    "\n",
    "**Similarity, Normalization and Scaling**\n",
    "\n",
    "The calculation of similarity is fundamental to making recommendations.  We typically calculate in two forms:\n",
    "\n",
    "A) Similarity between items  \n",
    "B) Similarity between users   \n",
    "\n",
    "One then makes recommendations for a user based on similar users or items similar to his likes.    \n",
    "\n",
    "**Similarity Metrics**    \n",
    "\n",
    "Many similarity metrics:\n",
    "\n",
    "* Jaccard is a similarity between sets of items   \n",
    "* Cosine distance is a similarity between two vectors   \n",
    "* Metrics can have a big effect  \n",
    "* Try many and see which make good predictions  \n",
    "\n",
    "The cosine distance is used most often. The cosine similarity between two vectors is a measure that calculates the cosine of the angle between them.   \n",
    "* Item attributes must be vectors  \n",
    "* Attribute vectors should be scaled     \n",
    "* Cosine Similarity values range from 1 to -1\n",
    "* Cosine similarity is a measure of similarity between two non-zero vectors  \n",
    "\n",
    "\n",
    "Advantages of cosine similarity:\n",
    "\n",
    "* Simple model based on linear algebra      \n",
    "* Term weights don't have to be binary     \n",
    "* Continuous degree of similarity   \n",
    "* Allows ranking of items   \n",
    "* Allows partial matching  \n",
    "\n",
    "\n",
    "\n",
    "Jaccard similarity is often used when comparing set overlap.  \n",
    "\n",
    "* Jaccard similarity is defined as the size of the intersection divided by the size of the union of the sample sets  \n",
    "\n",
    "_Question: How would one know which similarity metric is best?_      \n",
    "\n",
    "**Normalization**\n",
    "\n",
    "* Adjusting values to a common scale     \n",
    "* Can compare data on different scales   \n",
    "\n",
    "\n",
    "**Normalize by Z-score**\n",
    "\n",
    "* A z-score is the number of standard deviations from the mean    \n",
    "* Maps data to a standard normal distribution    \n",
    "* Allows one to easily assess how unusual a rating is   \n",
    "\n",
    "\n",
    "![Normalize by Z-score](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/The_Normal_Distribution.svg/800px-The_Normal_Distribution.svg.png)  \n",
    "\n",
    "\n",
    "Normalization of ratings means adjusting values measured on different scales to a notionally common scale, A z-score is the number of standard deviations from the mean. \n",
    " Normalizing by z-score maps data to a standard normal distribution. This allows one to easily assess how unusual a rating is.  \n",
    "\n",
    "Standardization by z-scores is a very common method. It converts all indicators to a common scale with an average of zero and standard deviation of one.\n",
    "\n",
    "Because a z-score is based on a normal probability distribution it also makes it much easier to assess how unusual a rating or recommendation is, such as a top 5% list. It is a good practice to normalize before calculating similarity.   \n",
    "\n",
    "\n",
    "**Denormalization**\n",
    "\n",
    "Denormalization allows one to get the ratings back in their original scale. When presenting rating users prefer the original scale rather than a z-score which they may not understand.   \n",
    "\n",
    "\n",
    "**Non-personalized Recommendations**\n",
    "\n",
    "Non-personalized recommendation systems recommend what is popular and relevant to all the users which can be a list of top-10 items for every new user.  This is critical for making recommendations to new users.  \n",
    "\n",
    "Another form of making recommendations to new users is called \"content-based\" recommendation systems, which will be discussed later.  \n",
    "\n",
    "\n",
    "**\"Cold Start\" Problem**\n",
    "\n",
    "The \"cold start\" problem refers the issue of how to make inferences for users or items about which it has not yet gathered sufficient information.\n",
    "\n",
    "\n",
    "**Popularity-based Filtering**\n",
    "\n",
    "* Sort on some statistic  \n",
    "* Most viewed, bought, liked, etc.  \n",
    "* Top rated    \n",
    "\n",
    "Useful in absence of any user information (i.e.\"Cold Start\" problem). In popularity-based filtering, one sorts on some statistic such as most viewed, bought, liked, etc. The most common rankings are A) top rated B) most popular and C) most used/bought. \n",
    "\n",
    "Popularity-based filtering is very useful in absence of any user information. This is called the \"Cold Start\" problem.  How do you make a recommendation for new users?\n",
    "\n",
    "One needs to be able to give recommendations to new users for which we had not collected any data. Popularity-based Filtering is a very common way of providing recommendations to new users.\n",
    "\n",
    "\n",
    "**Highest Average Rating**\n",
    "\n",
    "We can take s stab at non-personalized recommendations by hand by assuming better items receive on average better ratings.\n",
    "\n",
    "To calculate the highest average item rating we can just average the ratings and sort from high rating to low. This approach may not be very reliable for items with just a couple of ratings. The more ratings a item has to better its average reflects its quality so we may want to filter out jokes with very few ratings before taking an average.\n",
    "\n",
    "_Question: How can one calculate a confidence interval for an average item rating?_\n",
    "\n",
    "_Question: Why would one want to calculate a confidence interval for an average item rating?_\n",
    "\n",
    "\n",
    "**Most Often Rated Items**\n",
    "\n",
    "One might try another approach to non-personalized recommendations by assuming that more popular items are known to more people and receive more ratings. In effect, those popular items are good.  Of course, this approach would bias against newer items.\n",
    "\n",
    "\n",
    "_Question: How would one know whether \"Most Often Rated Items\" made better predictions than \"Highest Average Rating\"?_\n",
    "\n",
    "\n",
    "**How often is an item in the top 1%?**\n",
    "\n",
    "Yet another form of non-personalized recommendations might be to recommend items in the top X percent, for example, the top 1% or top 5%.  Data that is normalized by z-score makes this selection very easy.  \n",
    "\n",
    "A z-score greater 2.2 is the top 1%.  By selecting z-score normalized items above that threshold we get items in the top 1%.\n",
    "\n",
    "\n",
    "_Question: If there are 1000 items how many are in the top 1%?_   \n",
    "\n",
    "_Question: If items are ranked by average rating come up with a non z-score based algorithm to find items in the top 1%?_   \n",
    "\n",
    "_Question: What is the \"big-O\" of your algorithm?\"_   \n",
    "\n",
    "**Predictions**\n",
    "\n",
    "There are two types of predictions typically made by recommedners: \n",
    "\n",
    "1) top-N lists    \n",
    "2) ratings  \n",
    "\n",
    "_Question: When would you return a top-N list and when would you return a rating?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Recommenders\n",
    "\n",
    "**Content-based Filtering**\n",
    "\n",
    "Content-based filtering uses attributes of items to make recommendations.  \n",
    "\n",
    "* Items are the things to be recommended. These could be movies, music, web pages, tweets, books etc.  \n",
    "\n",
    "* Attributes are measurable characteristics of an item. These could be tags, genres, size, color, weight, etc.  \n",
    "\n",
    "Content-based filtering uses attributes of items to make recommendations. Items are the things to be recommended. These could be movies, music, web pages, tweets, books etc.\n",
    "\n",
    "An items attributes are measurable characteristics of an item. These could be tags, genres, etc.\n",
    "\n",
    "The idea is to use item content as features to calculate similarity.  The more relevant the content features are to aspects of the items that users like the better the recommendations.\n",
    "\n",
    "\n",
    "_Question: In what situations is content-based filtering likely to give good recommendations?_\n",
    "\n",
    "**Why use content-based filtering?**\n",
    "\n",
    "* New items can be recommended immediately, as no history of user ratings is required      \n",
    "* Results tend to be highly relevant as they rely on characteristics of the items themselves   \n",
    "* Recommendations are transparent, as the reasons for a recommendation is clear  \n",
    "\n",
    "\n",
    "New items can be recommended immediately, as no history of user ratings is required. This is the \"cold start\" problem.   \n",
    " The results tend to be highly relevant as they rely on the characteristics of the items. The recommendations are transparent, as the reasons for the recommendation is clear.\n",
    "\n",
    "The key for good content-based recommenders is that the content features are relevant to users tastes.  For example, in music genre and artist are highly relevant to users tastes. If one likes a country music song there is a good chance you'll like other country music.\n",
    "\n",
    "_Question: How would you know if content-based filtering makes good recommendations?_\n",
    "\n",
    "\n",
    "**Content Based Recommenders**\n",
    "\n",
    "* Measure \"how close\" items are  \n",
    "* Use content attributes as features  \n",
    "* There are many ways to measure the similarity  \n",
    "* The same items can produce different recommendations   \n",
    "\n",
    "Content-based recommenders measure \"how close\" items are and suggest similar items based on content features. This can be used to find items similar to an item.  The same items can produce different recommendations depending on which attributes are chosen and how similarity is measured.  \n",
    "\n",
    "\n",
    "**Issues with content-based filtering?**\n",
    "\n",
    "* Work involved in extracting the attributes   \n",
    "* Choice of similarity metric is not obvious   \n",
    "\n",
    "Sometimes a feature, like the price comes with the data. Often features are extracted, especially when working with text. By far, the bulk of the work in developing content-based recommenders is extracting relevant features to base the similarity calculations.  \n",
    "\n",
    "\n",
    "**Extracting Attributes**\n",
    "\n",
    "* Extracting important words from song track descriptions   \n",
    "* Using those important words from the song track   \n",
    "* Calculating the distance between songs   \n",
    "* Clustering songs into related groups  \n",
    "\n",
    "\n",
    "As an example of extracting important words from song track descriptions may be used as attributes. This would involve finding the important words. Then one calculates the distance between songs and clustering songs into related groups.   \n",
    "\n",
    "_Question: What attributes would be good features for a song recommender?_\n",
    "\n",
    "_Question: How would you know if an attribute is a good feature for a song recommender?_  \n",
    "\n",
    "**Distance between Songs**\n",
    "\n",
    "* Decide which item features to use  \n",
    "* Calculate the distance between items (songs)   \n",
    "* Recommend \"close\" items (songs)  \n",
    "\n",
    "The idea of a content-based recommender is simple: 1) decide which item features to use, 2) calculate the distance between items (songs), and 3) recommend \"close\" items (songs)\n",
    "\n",
    "The devil is in the details as there are many ways of calculating distance and many features that can be used to determine relevance.\n",
    "\n",
    "_Question: Name some distance and similarity metrics?_   \n",
    "\n",
    "\n",
    "_Question: Would the Jaccard or Euclidian metric make more sense for word counts?_\n",
    "\n",
    "_Question: Would the Cosine distance make sense for word counts?_   \n",
    "\n",
    "**Tf-Idf**\n",
    "\n",
    "One might extract \"important\" words from song descriptions/lyrics using Tf-Idf.   \n",
    "\n",
    "* TFIDF, short for term frequency-inverse document frequency, * Statistic that reflects word importance   \n",
    "* TF: Term Frequency, which measures how frequently a term occurs in a document   \n",
    "* IDF: Inverse Document Frequency, which measures how specific a term is    \n",
    "TF-IDF, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is in a corpus.  \n",
    "\n",
    "_Question: How else might one extract \"important\" words from song descriptions/lyrics?_   \n",
    "\n",
    "\n",
    "**Clustering Based Recommendations**   \n",
    "\n",
    "Once one has features one can cluster based on those features and then recommend other items from the same group.\n",
    "\n",
    "* Decide on relevant features   \n",
    "* Cluster items  \n",
    "* Recommend other items in the same cluster of a given item   \n",
    "\n",
    "\n",
    "To use clustering for content-based recommendations is straightforward: 1) We decide on relevant features, 2) We cluster items and 3) We recommend other items in the same cluster of a given item.  \n",
    "\n",
    "There are many clustering algorithms such as hierarchical clustering and K-means clustering.   \n",
    "\n",
    "_Question: If there are many other items from the same group how would one choose which to recommend?_  \n",
    "\n",
    "\n",
    "**Why clustering for content based recommendations?**\n",
    "\n",
    "* Distance based on attributes can be used to rank items by \"how close\" they are   \n",
    "* Distance based on attributes can also be used to cluster   \n",
    "* Recommend other items in the same cluster    \n",
    "\n",
    "Distance based on attributes can be used to rank items by \"how close\" they are. Distance based on attributes can also be used to cluster. Clustering is the task of grouping a set of items in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). We then recommend other items in the same cluster as an item of interest.   \n",
    "\n",
    "_Question: How would one choose a clustering algorithm?_   \n",
    "\n",
    "\n",
    "**Hierarchical Clustering**  \n",
    "\n",
    "Two types of hierarchical clustering:  \n",
    "\n",
    "* Agglomerative: This is a \"bottom-up\" approach  \n",
    "* Divisive: This is a \"top-down\" approach  \n",
    "\n",
    "Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. There are two types: 1) Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy and 2) Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.   \n",
    "\n",
    "\n",
    "**K-means Clustering**   \n",
    "\n",
    "* Partition n observations into k clusters   \n",
    "* Visualize the data before k-means clustering to see if there is a natural number of clusters       \n",
    "\n",
    "K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.  The choice of k is provided to the algorithm. Often one will visualize the data before k-means clustering to see if there is a natural number of clusters.\n",
    "\n",
    "The idea of k-means clustering for recommendations is to assign an item, like a song track to a group, then recommend other song tracks in that group. K-means clustering is less sensitive to small variations in the data like hierarchical clustering.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering   \n",
    "\n",
    "**What is collaborative filtering?**\n",
    "\n",
    "Collaborative filtering (CF) is an approach to making recommendations based on users’ past behavior. \n",
    "\n",
    "Two types of collaborative filtering:\n",
    "\n",
    "* Item-based: measure the similarity between the way users rate items   \n",
    "\n",
    "* User-based: measure the similarity between a user and other users  \n",
    "\n",
    "Collaborative filtering is an approach to making recommendations based on users’ past behavior. There are two types of collaborative filtering: 10 Item-based: measure the similarity between the way users rate items ans 2) User-based: measure the similarity between a user and other users. \n",
    "\n",
    "The idea behind collaborative filtering is that similar users share the same interest and will like similar items.\n",
    "\n",
    "\n",
    "See Machine Learning - Collaborative Filtering & Its Challenges [https://youtu.be/wTZUHiLUY94](https://youtu.be/wTZUHiLUY94)    \n",
    "\n",
    "_Question: Why would one use collaborative filtering rather than content based filtering?_    \n",
    "\n",
    "_Question: When can one use collaborative filtering?_   \n",
    "\n",
    "**Item-based Collaborative Filtering**   \n",
    "\n",
    "Item-based collaborative filtering:\n",
    "\n",
    "1. For every two items, measure how similar they are in terms of having received similar ratings by similar users   \n",
    "2. For each item, identify the k-most similar items    \n",
    "3. For each user, identify the items that are most similar to the user's items   \n",
    "\n",
    "Making Movie Recommendations with Item-Based Collaborative Filtering [https://youtu.be/Ty3Ui_mQtxY](https://youtu.be/Ty3Ui_mQtxY)  \n",
    "\n",
    "\n",
    "**User-based Collaborative Filtering**    \n",
    "\n",
    "• Measure how similar each user is to the new one       \n",
    "• Identify the most similar users (e.g. the top nearest neighbors) or whose similarity is above a defined threshold    \n",
    "• Rate the items purchased by the most similar users    \n",
    "• Pick the top-rated items  \n",
    "\n",
    "User-Based Collaborative Filtering [https://youtu.be/6mGMBipt7kU](https://youtu.be/6mGMBipt7kU)    \n",
    "\n",
    "_Question: How would you know whether user-based or item-based collaborative filtering was making better predictions?_   \n",
    "\n",
    "_Question: How could user-based or item-based collaborative filtering be used with new users?_   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Factorization**\n",
    "\n",
    "Matrix factorization models map both users and items to a latent factor space, such that user-item interactions are modeled as inner products in that space. In this form, matrix factorization characterizes both items and users by vectors of factors inferred from item rating patterns.  \n",
    "\n",
    "Singular value decomposition (SVD), is a very a common technique for identifying latent semantic factors in information retrieval. Applying SVD in the collaborative filtering domain requires factoring the user-item rating matrix. SVD finds the latent factors associated with some matrix. For example in recommender systems, the user-rating matrix of movies after an SVD, will decompose into matrices that represents latent user-user features and item-item features.   \n",
    "\n",
    "Note that SVD is only defined for complete matrices. So if you stick to true SVD you need to fill in these missing values before performing singular value decomposition (SVD).   \n",
    "\n",
    "Note that in recommender systems the method usually referred to as \"SVD\" that is used in the context of recommendations is not strictly speaking the mathematical singular value decomposition of a matrix but rather an approximate way to compute the low-rank approximation of the matrix by minimizing the squared error loss. The basic idea is the same as SVD, we want to decompose our original and very sparse matrix into two low-rank matrices that represent user factors and item factors. A better name for it would be to call it matrix factorization, but term \"SVD\" is widely used.  \n",
    "\n",
    "See [https://en.wikipedia.org/wiki/Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)   \n",
    "\n",
    "Using Singular Value Decomposition (SVD) for Movie Recommendations [https://youtu.be/8wLKuscyO9I](https://youtu.be/8wLKuscyO9I)  \n",
    "\n",
    "\n",
    "Latent Factor Recommender System  | Stanford University [https://youtu.be/E8aMcwmqsTg](https://youtu.be/E8aMcwmqsTg)   \n",
    "\n",
    "\n",
    "Singular Value Decomposition | Stanford University [https://youtu.be/P5mlg91as1c ](https://youtu.be/P5mlg91as1c)  \n",
    "\n",
    "\n",
    "_Question: How would one create training and test data using a sparse user-items ratings matrix?_\n",
    "\n",
    "_Question: User-items ratings matrix tend to be very sparse, how can one singular value decomposition (SVD) on them?_\n",
    "\n",
    "_Question: What are the advantages of reconstructing a user-item ratings matrix from low-rank approximations?_\n",
    "\n",
    "\n",
    "**Funk SVD, SVD++, etc.**\n",
    "\n",
    "Latent factor models tend to make great recommendations but there are two big issues:\n",
    "\n",
    "1. The methods only work for complete matrices   \n",
    "2. It is compuationally expensive and user-item ratings matrix can be very large  \n",
    "\n",
    "Other algorithms compute the low-rank approximation of the user and items matrices but try to better impute missing data and allow for the addition of new data iteratively so that the complete matrices need to be re-computing.\n",
    "\n",
    "Some very popular approaches are Funk SVD, and SVD++.\n",
    "\n",
    "See the following for more info.\n",
    "\n",
    "SVD++ Linearity [https://youtu.be/AENGiI-QTN0](https://youtu.be/AENGiI-QTN0)   \n",
    "\n",
    "\n",
    "What's the difference between SVD and SVD++?  [https://qr.ae/TUhLay](https://qr.ae/TUhLay)   \n",
    "\n",
    "_Question: How would you know if Funk SVD, or SVD++ was \"better\" than SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Eigenvalues and Eigenvectors   \n",
    "\n",
    "\n",
    "To understand the math of matrix factorization we need to nderstand the math of dimensionality reduction, Eigenvalues and Eigenvectors. Dimensionality reduction is about converting data of high dimensionality into data of lower dimensionality while keeping most of the information in the data. This allows us to work on larger datasets and identify the data’s most relevant features. Anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset.  In this first lesson, we study the theory of dimensionality reduction and introduce essential concepts and jargon, such as eigenvalues, eigenvectors, linear independence, span, vector, scalar, basis of a subspace and linear combination.   \n",
    "\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "In machine learning and statistics, [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) or dimension reduction is the process of reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction.\n",
    "\n",
    "*Feature selection*  \n",
    "\n",
    "Feature selection approaches try to find a subset of the original variables (also called features or attributes). In essence, either using domain knowledge and statisitcal tests to prune away some of the original variables\n",
    "\n",
    "*Feature extraction*  \n",
    "\n",
    "Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions.  \n",
    "\n",
    "\n",
    "## Factor Analysis \n",
    "\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  The observed variables are modelled as linear combinations of the potential factors, plus \"error\" terms. \n",
    "\n",
    "Factor Analysis has was first developed in 1901 by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson). Pearson posed a model having one factor that was common across his data:\n",
    "\n",
    "$$\n",
    "Y_ij = \\alpha_i W_1 + \\varepsilon_{ij}\n",
    "$$\n",
    "\n",
    "Or a general multi-factor factor:\n",
    "\n",
    "$$\n",
    "Y_{ij} = \\sum_{k=1}^K \\alpha_{i,k} W_{j,k} + \\varepsilon_{ij}\n",
    "$$\n",
    "\n",
    "We can use various techniques to estimate $\\mathbf{W}_1,\\dots,\\mathbf{W}_K$. Choosing $k$ (the number of factors) is a challenge that we'll discuss in further sections.\n",
    "\n",
    "To illustrate the idea of feature extraction using factors consider the problem of reducing 2D data to 1D.\n",
    "\n",
    "\n",
    "![image 2D data to 1D A](http://54.198.163.24/YouTube/MachineLearning/M03/Eigin_Projection_A.png)     \n",
    "\n",
    "![image 2D data to 1D B](http://54.198.163.24/YouTube/MachineLearning/M03/Eigin_Projection_B.png)   \n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "So how do we find a good basis to project some data?\n",
    "\n",
    "### Some Linear Algebra Jargon\n",
    "\n",
    "#### Real coordinate spaces  \n",
    "\n",
    "$$\\mathbb{R}^2 , \\quad \\mathbb{R}^3, ... , \\quad \\mathbb{R}^N$$\n",
    "\n",
    "#### What is a vector?\n",
    "\n",
    "* A vector is a quantity having direction as well as magnitude.\n",
    "* An element of the real coordinate space $\\mathbb{R}^N$\n",
    "\n",
    "#### Systems of Linear Equations\n",
    "\n",
    "Linear algebra are used to solve systems of linear equations such as this:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a + b + c &= 5\\\\\n",
    "3a - 2b + c &= 3\\\\\n",
    "2a + b  - c &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can rewrite and solve this system using matrix algebra notation:\n",
    "\n",
    "$$\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\implies\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Multiplying by Vector or Matrix by a Scalar\n",
    "\n",
    "scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction.\n",
    "\n",
    "![image Scalar multiplication A](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Scalar_multiplication_by_r%3D3.svg/500px-Scalar_multiplication_by_r%3D3.svg.png)     \n",
    "\n",
    "\n",
    "Multiplying by a negative value changes its direction.\n",
    "\n",
    "\n",
    "![image Scalar multiplication A](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Scalar_multiplication_of_vectors2.svg/500px-Scalar_multiplication_of_vectors2.svg.png)     \n",
    "  \n",
    "\n",
    "If $a$ is scalar and $\\mathbf{X}$ is a matrix then:\n",
    "\n",
    "$$\n",
    "a \\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "a x_{1,1} & \\dots & a x_{1,p}\\\\\n",
    "& \\vdots & \\\\\n",
    "a x_{N,1} & \\dots & a  x_{N,p}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Properties of Scalar Multiplication  \n",
    "\n",
    "Scalar multiplication obeys the following rules:  \n",
    "* Additivity in the scalar: $(c + d)\\vec{v} = c\\vec{v} + d\\vec{v};$  \n",
    "* Additivity in the vector: $c(\\vec{v} + \\vec{w}) = c\\vec{v} + c\\vec{w};$  \n",
    "* Compatibility of product of scalars with scalar multiplication: $(cd)\\vec{v} = c(d\\vec{v});$  \n",
    "* Multiplying by 1 does not change a vector: $1\\vec{v} = \\vec{v};$  \n",
    "* Multiplying by 0 gives the zero vector: $0\\vec{v} = 0;$  \n",
    "* Multiplying by −1 gives the additive inverse: $(−1)\\vec{v} = −\\vec{v}.$  \n",
    "\n",
    "#### The Matrix Transpose\n",
    "\n",
    "The matrix transpose is an operation that changes columns to rows. We use either a $\\top$ to denote transpose.   \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "x_{1,1}&\\dots & x_{1,p} \\\\\n",
    "x_{2,1}&\\dots & x_{2,p} \\\\\n",
    "& \\vdots & \\\\\n",
    "x_{N,1}&\\dots & x_{N,p}\n",
    "\\end{pmatrix} \\implies\n",
    "\\mathbf{X}^\\top = \\begin{pmatrix}\n",
    "x_{1,1}&\\dots & x_{p,1} \\\\\n",
    "x_{1,2}&\\dots & x_{p,2} \\\\\n",
    "& \\vdots & \\\\\n",
    "x_{1,N}&\\dots & x_{p,N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Matrix multiplication\n",
    "\n",
    "[matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) is an operation that takes a pair of matrices, and produces another matrix. If A is an n × m matrix and B is an m × p matrix, their matrix product AB is an n × p matrix. The number rows of the first matrix must match the columns of the second.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a + b + c &=5\\\\\n",
    "3a - 2b + c &= 3\\\\\n",
    "2a + b  - c &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The idea is to multiply the rows of the first matrix by the columns of the second.\n",
    "\n",
    "$$\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "a + b + c \\\\\n",
    "3a - 2b + c \\\\\n",
    "2a + b  - c\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "##### Adding vectors\n",
    "\n",
    "\n",
    "![image vector addition A](http://54.198.163.24/YouTube/MachineLearning/M03/Vector_addition_M03.png)   \n",
    "\n",
    "\n",
    "#### Unit vector\n",
    "\n",
    "A unit vector in a normed vector space is a vector of length 1. A unit vector is often denoted by a lowercase letter with a \"hat\": i-hat (pronounced \"i-hat\"). The normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,\n",
    "\n",
    "$$\\mathbf{\\hat{u}} = \\frac{\\mathbf{u}}{\\|\\mathbf{u}\\|}$$\n",
    "\n",
    "u-hat equals the vector u divided by its length where ||u|| is the norm (or length) of u. The term normalized vector is sometimes used as a synonym for unit vector.\n",
    "\n",
    "from [Unit vector - Wikipedia](https://en.wikipedia.org/wiki/Unit_vector)\n",
    "\n",
    "#### Linear combinations \n",
    "\n",
    "We can represent any vector with a linear combination of other vectors. If $\\vec{V}$ are vectors $v_1,...,v_n$ and A are scalars $a_1,...,a_n$ then the linear combination of those vectors with those scalars as coefficients is\n",
    "\n",
    "$$ a_1 \\vec{v}_{1} \\quad + \\quad a_2 \\vec{v}_2  \\quad + \\quad  a_3 \\vec{v}_3  \\quad + \\quad  \\cdots  \\quad + \\quad  a_n \\vec{v}_n. $$  \n",
    "\n",
    "For example, consider the vectors $\\vec{v}_1 = (1,0,0),  \\quad \\vec{v}_2 = (0,1,0)  \\quad and  \\quad \\vec{v}_3 = (0,0,1)$. Then any vector in $\\mathbb{R}^3$ is a linear combination of $\\vec{v}_1,  \\quad \\vec{v}_2  \\quad and  \\quad \\vec{v}_3$.\n",
    "To see that this is so, take an arbitrary vector (a1,a2,a3) in $\\mathbb{R}^3,$, and write:\n",
    "\n",
    "$$ ( a_1 , a_2 , a_3) = ( a_1 ,0,0) + (0, a_2 ,0) + (0,0, a_3)   =  a_1 (1,0,0) + a_2 (0,1,0) + a_3 (0,0,1) =  a_1 \\vec{v}_1 +  a_2 \\vec{v}_2 +  a_3 \\vec{v}_3.   $$\n",
    "\n",
    "![image Linear combination Standard basis A](http://54.198.163.24/YouTube/MachineLearning/M03/3D_Vector_Linear_Combo.png)   \n",
    "\n",
    "#### Linear Spans and Basis\n",
    "\n",
    "The span of S may be defined as the set of all finite linear combinations of elements of S,\n",
    "\n",
    "$$ \\operatorname{span}(S) =  \\left \\{ {0+\\sum_{i=1}^k \\lambda_i v_i \\Big| k \\in \\mathbb{N}, v_i  \\in S, \\lambda _i  \\in \\mathbf{K}} \\right \\} $$  \n",
    "\n",
    "A set of vectors in a vector space V is called a [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra)), or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set. In more general terms, a basis is a linearly independent spanning set.\n",
    "\n",
    "For example, the real vector space $\\mathbb{R}^3$ has {(5,0,0), (0,3,0), (0,0,9)} is a spanning set. This particular spanning set is also a basis. \n",
    "\n",
    "The set {(1,0,0), (0,1,0), (1,3,0)} is not a spanning set of $\\mathbb{R}^3$ as a vector like (1,3,1) cannot be created from this spanning set.\n",
    "\n",
    "#### Linear subspaces\n",
    "\n",
    "A [linear subspace](https://en.wikipedia.org/wiki/Linear_subspace) (or vector subspace) is a vector space that is a subset of some other (higher-dimension) vector space. For example, $\\mathbb{R}^2$ is a subspace of $\\mathbb{R}^3$.\n",
    " \n",
    "Let V be a vector space over the field K, and let W be a subset of V. Then W is a subspace if and only if W satisfies the following three conditions:  \n",
    "\n",
    "*  The zero vector, 0, is in W.\n",
    "*  If u and v are elements of W, then the sum u + v is an element of W;\n",
    "*  If u is an element of W and c is a scalar from K, then the product cu is an element of W;\n",
    "\n",
    "\n",
    "#### Metric Spaces\n",
    "\n",
    "A metric space is an ordered pair (M,d) where M is a set and d is a metric on M, i.e., a function * $d \\colon M \\times M \\rightarrow \\mathbb{R}$\n",
    "such that for any * $x, y, z \\in M$, the following holds:[1]\n",
    "\n",
    "* $d(x,y) \\ge 0$     (non-negative),  \n",
    "* $d(x,y) = 0\\, \\iff x = y\\,$     (identity of indiscernibles),  \n",
    "* $d(x,y) = d(y,x)\\,$     (symmetry) and  \n",
    "* $d(x,z) \\le d(x,y) + d(y,z) $    (triangle inequality) .  \n",
    "\n",
    "Examples of metric spaces include  [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry), [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance), the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and many others.\n",
    "\n",
    "#### Vector dot product\n",
    "\n",
    "*Algebraic definition*\n",
    "\n",
    "The dot product of two vectors A = [A1, A2, ..., An] and B = [B1, B2, ..., Bn] is defined as:\n",
    "\n",
    "$$ \\mathbf{A}\\cdot \\mathbf{B} = \\sum_{i=1}^n A_iB_i = A_1B_1 + A_2B_2 + \\cdots + A_nB_n $$ \n",
    "\n",
    "\n",
    "*Geometric definition*  \n",
    "\n",
    "he magnitude of a vector A is denoted by  \\left\\| \\mathbf{A} \\right\\| . The dot product of two Euclidean vectors A and B is:\n",
    "\n",
    "\n",
    "$$\\mathbf A \\cdot \\mathbf B = \\left\\| \\mathbf A \\right\\| \\, \\left\\| \\mathbf B \\right\\| \\cos \\theta ,$$\n",
    "\n",
    "where $\\theta$ is the angle between A and B.\n",
    "\n",
    "![image dot product  A](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/440px-Dot_Product.svg.png)  \n",
    "\n",
    "\n",
    "#### Orthogonal and orthonormal\n",
    "\n",
    "Two vectors are orthogonal if they are perpendicular, i.e., they form a right angle. Two vectors orthonormal if they are orthogonal and unit vectors. Orthogonal vectors in n-space if their dot product equals zero. That is, if A and B are orthogonal, then the angle between them is 90° and $\\mathbf A \\cdot \\mathbf B = 0 .$\n",
    "\n",
    "\n",
    "## Eigenvalues and eigenvectors\n",
    "\n",
    "In linear algebra, an eigenvector or characteristic vector of a square matrix is a vector that does not change its direction under the associated linear transformation. In other words—if v is a vector that is not zero, then it is an eigenvector of a square matrix A if Av is a scalar multiple of v. This condition could be written as the equation\n",
    "\n",
    "$$A\\vec{v} = \\lambda \\vec{v}$$\n",
    " \n",
    "\n",
    "where $\\lambda$ is a number (also called a scalar) known as the eigenvalue or characteristic value associated with the eigenvector $\\vec{v}$.   \n",
    "\n",
    "![image Mona Lisa eigenvector](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Mona_Lisa_eigenvector_grid.png/480px-Mona_Lisa_eigenvector_grid.png)   \n",
    "\n",
    "In this shear mapping the red arrow changes direction but the blue arrow does not. The blue arrow is an eigenvector of this shear mapping because it doesn't change direction, and since its length is unchanged, its eigenvalue is 1.  \n",
    "\n",
    "For example, Consider n-dimensional vectors that are formed as a list of n real numbers, such as the three dimensional vectors,\n",
    "\n",
    "$$ \n",
    "\\mathbf{u} = \\begin{Bmatrix}1\\\\3\\\\4\\end{Bmatrix}\\quad\\mbox{and}\\quad \\mathbf{v} = \\begin{Bmatrix}-20\\\\-60\\\\-80\\end{Bmatrix}. $$ \n",
    "\n",
    "These vectors are said to be scalar multiples of each other, also parallel or collinear, if there is a scalar λ, such that\n",
    "\\mathbf{u}=\\lambda\\mathbf{v}.\n",
    "\n",
    "In this case $\\lambda\\$ = −1/20.  \n",
    "\n",
    "Now consider the linear transformation of n-dimensional vectors defined by an n×n matrix A, that is,\n",
    "\n",
    "$$\n",
    " A\\mathbf{v}=\\mathbf{w},\n",
    "or\n",
    "\\begin{bmatrix} A_{1,1} & A_{1,2} & \\ldots & A_{1,n} \\\\\n",
    "A_{2,1} & A_{2,2} & \\ldots & A_{2,n} \\\\\n",
    "\\vdots &  \\vdots &  \\ddots &  \\vdots \\\\\n",
    "A_{n,1} & A_{n,2} & \\ldots & A_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{Bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{Bmatrix} = \\begin{Bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{Bmatrix}\n",
    "where, for each index i,\n",
    " w_i = A_{i,1} v_1 + A_{i,2} v_2 + \\cdots + A_{i,n} v_n = \\sum_{j = 1}^{n} A_{i,j} v_j.\n",
    " \n",
    " $$\n",
    "If it occurs that w and v are scalar multiples, that is if\n",
    "$A\\mathbf{v}=\\lambda\\mathbf{v}$,\n",
    "then v is an eigenvector of the linear transformation A and the scale factor $\\lambda\\$ is the eigenvalue corresponding to that eigenvector.\n",
    "\n",
    "\n",
    "![image eigenvector](https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/500px-Eigenvalue_equation.svg.png)   \n",
    "\n",
    "Matrix A acts by stretching the vector x, not changing its direction, so x is an eigenvector of A.\n",
    "\n",
    "from [Eigenvalues and eigenvectors - Wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)\n",
    "\n",
    "\n",
    "## PCA: Principal Component Analyses\n",
    "\n",
    "[Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "In Principal component analysis the feature selection is finding a set of linearly uncorrelated vectors (basis selection) of maximal variance. That is, the transformation is defined in such a way that the first principal component has the largest possible variance and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. \n",
    "\n",
    "![image  Principal Component Analyses](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/440px-GaussianScatterPCA.png)  \n",
    "\n",
    "*First component*  \n",
    "\n",
    "The first loading vector $\\vec{w}$ thus has to satisfy  \n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(t_1\\right)^2_{(i)} \\right\\}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(\\mathbf{x}_{(i)} \\cdot \\mathbf{w} \\right)^2 \\right\\} $$ \n",
    " \n",
    "Equivalently, writing this in matrix form gives\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\{ \\Vert \\mathbf{Xw} \\Vert^2 \\}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\left\\{ \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X w} \\right\\} $$  \n",
    " \n",
    " \n",
    "Since $\\vec{w}$ has been defined to be a unit vector, it equivalently also satisfies  \n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\frac{\\mathbf{w}^T\\mathbf{X}^T \\mathbf{X w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\}\n",
    "$$\n",
    "\n",
    "The first component is an eigenvector that maximizes the sum of squares\n",
    "\n",
    "$$(\\mathbf{Yv}_1)^\\top \\mathbf{Yv}_1$$\n",
    "\n",
    "$\\mathbf{v}_1$ is referred to as the _first principal component_ (PC). Also referred as  _first eigenvector_, $\\mathbf{Yv}_1$ are the projections or coordinates or eigenvalues\n",
    "\n",
    "*Further components*  \n",
    "\n",
    "The kth component can be found by subtracting the first k − 1 principal components from X:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{X}}_{k} = \\mathbf{X} - \\sum_{s = 1}^{k - 1} \\mathbf{X} \\mathbf{w}_{(s)} \\mathbf{w}_{(s)}^{\\rm T} $$  \n",
    "\n",
    "and then finding the loading vector which extracts the maximum variance from this new data matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(k)} = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{arg\\,max}} \\left\\{ \\Vert \\mathbf{\\hat{X}}_{k} \\mathbf{w} \\Vert^2 \\right\\} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\tfrac{\\mathbf{w}^T\\mathbf{\\hat{X}}_{k}^T \\mathbf{\\hat{X}}_{k} \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\} $$  \n",
    "\n",
    "It turns out that this gives the remaining eigenvectors of $X^TX$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues.\n",
    "\n",
    "The kth  is the vector that\n",
    "\n",
    "$$ \\mathbf{v}_{k}^\\top \\mathbf{v}_{k}=1$$\n",
    "\n",
    "$$ \\mathbf{v}_{k}^\\top \\mathbf{v}_{k-1}=0$$\n",
    "\n",
    "and maximizes  $$(\\mathbf{rv}_{k})^\\top \\mathbf{rv}_{k}$$\n",
    "\n",
    "\n",
    "#### Properties of Principal Components\n",
    "\n",
    "* The principal components are eigenvectors\n",
    "* Maximizes variance in order of the components\n",
    "* They are orthogonal (and form a basis)\n",
    "* If use all of the principal components  we can get lossless reconstruction \n",
    "* N diminsions to M diminsions (each diminsion has an eigenvalue \n",
    "the order (highest eigenvalues have highest  variance)\n",
    "i.e. can \"throw away\" the lowest eigenvalues.\n",
    "if eigenvalue has 0 value can throw it away without cost.\n",
    "* We can readjust to origin 0\n",
    "* There are very fast algorithms to compute PCA\n",
    "\n",
    "*Nota bene*  \n",
    "Note that there may be oroblems of interpretation of the components.\n",
    "Note that a low variance dimension may be important\n",
    "\n",
    "\n",
    "### LDA: Linear Discriminant Analysis\n",
    "\n",
    "\n",
    "LDA: Linear Discriminant Analysis\n",
    "\n",
    "An alternative method to calculate eigenvectors from covariance matrix\n",
    "[Linear discriminant analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n",
    "\n",
    "\n",
    "Singular Value Decomposition\n",
    "\n",
    "In linear algebra, the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is a factorization of a real or complex matrix. A  a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices.\n",
    "\n",
    "If $\\mathbf{M}$ is a m × n matrix whose entries come from $\\mathbb{R}$, then the SVD  a factorization, called a singular value decomposition of  $\\mathbf{M}$ , of the form $\\mathbf{M=UDV}^\\top$ gives the factors in columns of $\\mathbf{V}$.\n",
    "\n",
    "Note that,  \n",
    "$\\mathbf{D}$ is a m × n diagonal matrix with non-negative real numbers on the diagonal, and  \n",
    "$\\mathbf{U}$  is an m × m, and $\\mathbf{V}$ is an n × n, unitary matrix over $\\mathbb{R}$.\n",
    "\n",
    "The diagonal entries, $\\sigma_{i}$, of $D$ are known as the singular values of $\\mathbf{M}$. The singular values are usually listed in descending order.\n",
    "\n",
    "Note that [Eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) is a form of matrix factorization.   \n",
    "\n",
    "$A=VDV^{-1}$, where D is a diagonal matrix formed from the eigenvalues of A, and the columns of V are the corresponding eigenvectors of A.\n",
    "\n",
    "#### Differences between Principal Component Analysis and Singular-value decomposition\n",
    "\n",
    "The singular value decomposition and the eigendecomposition are closely related.  PCA viewpoint requires that one compute the eigenvalues and eigenvectors of the covariance matrix, which is the product $\\mathbf{X}\\mathbf{X^T}$, where $\\mathbf{X}$ is the data matrix and SVD constructs the covariance matrix from this decomposition\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{(UDV^T)}\\mathbf{(UDV^T)}^T$$\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{(UDV^T)}\\mathbf{(VDU^T)}$$  \n",
    "\n",
    "and since $\\mathbf{V}$ is an orthogonal matrix $\\mathbf{V^TV}=\\mathbf{I}$),\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{U}\\mathbf{D}^2\\mathbf{U^T}$$\n",
    "\n",
    "That is,  \n",
    "- The left-singular vectors of $\\mathbf{M}$ are eigenvectors of $\\mathbf{M}\\mathbf{M^*}$ .  \n",
    "- The right-singular vectors of  $\\mathbf{M}$ are eigenvectors of $\\mathbf{M^*}\\mathbf{M}$.  \n",
    "- The non-zero singular values of $\\mathbf{M}$  (found on the diagonal entries of $\\mathbf{D}$) are the square roots of the non-zero eigenvalues of both $\\mathbf{M^*}\\mathbf{M}$ and $\\mathbf{M}\\mathbf{M^*}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommenders \n",
    "\n",
    "**Hybrid Recommenders**\n",
    "\n",
    "Hybrid recommenders:\n",
    "*  Most common form of “real world” recommender systems    \n",
    "*  Various models have their strengths and weaknesses   \n",
    "*  Hybrid recommenders allow the use of many recommender models  \n",
    "\n",
    "\n",
    "Hybrid recommenders are the most common form of “real world” recommender systems. Various models have their strengths and weaknesses. Some models suffer from the “cold start” problem and can’t be used without user data. Some data is better suited to content-based recommenders. All recommenders have bias. Often aggregate recommenders outperform individual models. \n",
    "\n",
    "\n",
    "_Question: How would one know if an aggregate recommenders outperformed the individual models?_  \n",
    "\n",
    "\n",
    "**Hybrid Recommender Creation**\n",
    "\n",
    "Hybrid recommenders are built by:\n",
    "1. Creating individual recommenders   \n",
    "2. Aggregating their predictions  \n",
    "\n",
    "Hybrid recommenders are built by first creating individual recommenders and then aggregating their predictions. \n",
    "\n",
    "If a prediction matrix has columns that represent the predictions that recommender model makes for each row, a user, one can add additional columns that are some form of aggregating the other columns. For example, by averaging the predictions from a UBCF and IBCF into another UBCF_IBCF prediction column.  \n",
    "\n",
    "_Question: Would it be easier to aggregate top-N lists or ratings?_    \n",
    "\n",
    "\n",
    "**Aggregate Predictions**\n",
    "\n",
    "Aggregating predictions:\n",
    "1. A user-item prediction matrix is created with an additional rating for each recommender models \n",
    "2. An aggregate \"hybrid rating\" is created  \n",
    "3. Numeric ratings are easier to work with  \n",
    "4. Calculating the mean rating is common  \n",
    "\n",
    "\n",
    "The second step in creating hybrid recommenders is to aggregate the predictions amongst the individual recommender models. Predictions based on ratings are numeric and easily aggregated by taking the mean rating over a set of model rating predictions.\n",
    "\n",
    "More sophisticated aggregation schemes can be used to weight some models more heavily than others, but in this exercise, we will simply calculate the mean rating over a set of model rating predictions.\n",
    "\n",
    "The following steps are used in aggregating predictions: 1) A user-item prediction matrix is created with an additional rating for each recommender models, 2) An aggregate \"hybrid rating\" is created, 3) Numeric ratings are easier to work with and 4.) Calculating the mean rating is common.\n",
    "\n",
    "\n",
    "**Getting top-N lists from Hybrid Predictions**\n",
    "\n",
    "Getting top-n lists from a user-item ratings matix:\n",
    "1. Get numeric ratings for a user  \n",
    "2. Sort them  \n",
    "3. Take the top-N \n",
    "\n",
    "Note the the primary data structure will be numeric user-item ratings matices.  If one wants top-N lists we must derive them from the numeric ratings.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Recommender Models\n",
    "\n",
    "**Recommender Model Evaluation**\n",
    "\n",
    "Out of sample validation:\n",
    "\n",
    "* Cross-validation    \n",
    "* Test/training splits  \n",
    "\n",
    "_Question: Must we test training data by sampling rows (i.e. users)?_  \n",
    "\n",
    "\n",
    "Recommender model evaluation is done with standard out of sample validation, cross-validation, and test/training splits.  The main difference is that we may sample individual ratings rather than rows.  \n",
    "\n",
    "**Cross-validation**\n",
    "\n",
    "\n",
    "* k-fold cross-validation  \n",
    "* Data randomly partitioned into k equal sized subsamples\n",
    "* The k results can then be averaged to produce a single estimation \n",
    "* Generally better estimates than a single test training split\n",
    "\n",
    "\n",
    "In k-fold cross-validation, the data is randomly partitioned into k roughly equal sized subsamples. One of the k sets is held out for testing and the other k-1 sets are used for training. The k results are averaged to produce a single estimation.\n",
    "\n",
    "\n",
    "**Test/training Splits**\n",
    "\n",
    "Test/training splits:\n",
    "\n",
    "* Randomly split data in to a test and training set.\n",
    "* Usually 80/20 or 90/10 training versus test percentages.\n",
    "* In most cases, n-folds cross-fold validation gives better estimates.  \n",
    "\n",
    "The primary purpose of splitting into training and test sets is to verify how well would your model perform on unseen data.\n",
    "\n",
    "\n",
    "**Evaluation Schemes**\n",
    "\n",
    "As we are comparing many models one uses the same validtion for all the models.  This is often called an \"evaluation schemes.\" For example, use a method of cross-validation with a k of 3 for all recommender models.   \n",
    "\n",
    "**Top-N Evaluation Schemes**\n",
    "\n",
    "Remember that when we evaluate top-n lists then this is a classification problem and we use classification metrics.\n",
    "\n",
    "In addition, there is the twist that we must specify the n for a top-n list.  One typically choose a top-n list with many n, say n=1, 3, 5, 10.  This would mean validating on top-1, top-3, top-5 and top-10 lists.\n",
    "\n",
    "_Question: Why would one bother evaluating models on top-n lists when we have the numeric user-item ratings matrix data?_  \n",
    "\n",
    "_Classification metrics_    \n",
    "\n",
    "misclassification  \n",
    "This is the overall error, the number shown in the bottom right of a confusion matrix. If it got 1 of 20 wrong in class A, 1 of 50 wrong in class B, and 2 of 30 wrong in class C, it got 4 wrong in total out of 100, so the misclassification is 4, or 4%.\n",
    " \n",
    "mean per class error  \n",
    "The right column in a confusion matrix has an error rate for each class. This is the average of them, so for the preceding example it is the mean of 1/20, 1/50, nd 2/30, which is 4.556%. If your classes are balanced (exactly the same size) it is identical to misclassification.\n",
    " \n",
    "logloss  \n",
    "A probability for the answer being each category. The confidence assigned to the correct category is used to calculate logloss (and MSE). Logloss disproportionately punishes low numbers, which is another way of saying having high confidence in the wrong answer is a bad thing.\n",
    " \n",
    "MSE  \n",
    "Mean Squared Error. The error is the distance from 1.0 of the probability it suggested. So assume we have three classes, A, B, and C, and your model guesses A with 0.91, B with 0.07, and C with 0.02. If the correct answer was A the error (before being squared) is 0.09, if it is B 0.93, and if C it is 0.98.\n",
    " \n",
    "AUC \n",
    "Area Under Curve. \n",
    " \n",
    "See (https://www.youtube.com/watch?v=OAl6eAyP-yo)[https://www.youtube.com/watch?v=OAl6eAyP-yo\n",
    "]  \n",
    "\n",
    "\n",
    "**Ratings Evaluations**\n",
    "\n",
    "Remember that when we evaluate ratings then this is a regression problem and we use regression metrics.\n",
    "\n",
    "This is simpler and faster than extracting the top-n lists and evaluating recommender models for top-n list with many n.\n",
    "\n",
    "\n",
    "_Regression metrics_  \n",
    "\n",
    "MSE  \n",
    "Mean Squared Error. The “squared” bit means the bigger the error, the more it is punished. If your correct answers are 2,3,4 and your algorithm guesses 1,4,3, the absolute error on each one is exactly 1, so squared error is also 1, and the MSE is 1. But if your algorithm guesses 2,3,6, the errors are 0,0,2, the squared errors are 0,0,4, and the MSE is a higher 1.333.   \n",
    " \n",
    "deviance   \n",
    "Actually short for mean residual deviance. If the distribution is gaussian, then it is equal to MSE, and when not it usually gives a more useful estimate of error, which is why it is prefered to MSE.  \n",
    " \n",
    " \n",
    "RMSE  \n",
    "The square root of MSE. If your response variable units are dollars, the units of MSE is dollars-squared, but RMSE is back into dollars.  \n",
    " \n",
    "MAE  \n",
    "Mean Absolute Error. Following on from the MSE example, a guess of 1,4,3 has absolute errors of 1, so the MAE is 1. But 2,3,6 has absolute errors of 0,0,2 so the MAE is 0.667. As with RMSE, the units are the same as your response variable.  \n",
    " \n",
    "R2  \n",
    "R-squared, also written as R., and also known as the coefficient of determination.  \n",
    " \n",
    "  \n",
    "RMSLE  \n",
    "The catchy abbreviation of Root Mean Squared Logarithmic Error. Prefer this to RMSE if an under-prediction is worse than an over-prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review  \n",
    "\n",
    "**Many Types of Recommenders**\n",
    "\n",
    "* Popularity based     \n",
    "* Content based   \n",
    "* Collaborative filtering     \n",
    "* Latent factor models  \n",
    "* more ....   \n",
    "\n",
    "All have strengths and weaknesses\n",
    "\n",
    "**Popularity-based Filtering**  \n",
    "\n",
    "Popularity-based filtering:\n",
    "\n",
    "* Sort on some statistic\n",
    "* Most viewed, bought, liked, etc. \n",
    "* Top rated  \n",
    "\n",
    "Useful in absence of any user information (i.e.\"Cold Start\" problem)\n",
    "\n",
    "\n",
    "**Content-based Filtering**  \n",
    "\n",
    "Content-based filtering uses attributes of items to make recommendations.  \n",
    "\n",
    "* Items are the things to be recommended. These could be movies, music, web pages, tweets, books etc.\n",
    "\n",
    "* Attributes are measurable characteristics of an item. These could be tags, genres, size, color, weight,etc.\n",
    "\n",
    "Useful when attributes are relevant to all user preferences. \n",
    "\n",
    "\n",
    "**Collaborative Filtering**\n",
    "\n",
    "Collaborative Filtering (CF) is an approach to making recommendations based on users’ past behavior. \n",
    "\n",
    "Two types of collaborative filtering:\n",
    "\n",
    "* Item-based: measure the similarity between the way users rate items  \n",
    "\n",
    "* User-based: measure the similarity between a user and other users  \n",
    "\n",
    "The idea behind collaborative filtering is that similar users share the same interest and will like similar items.  \n",
    "\n",
    "\n",
    "**Item-based Collaborative Filtering**\n",
    "\n",
    "Item-based collaborative filtering:\n",
    "\n",
    "1. For every two items, measure how similar they are in terms of having received similar ratings by similar users \n",
    "2. For each item, identify the k-most similar items  \n",
    "3. For each user, identify the items that are most similar to the user's items    \n",
    "\n",
    "**User-based Collaborative Filtering**  \n",
    "\n",
    "User-based collaborative filtering\n",
    "\n",
    "• Measure how similar each user is to the new one       \n",
    "• Identify the most similar users (e.g. the top nearest neighbors) or whose similarity is above a defined threshold     \n",
    "• Rate the items purchased by the most similar users    \n",
    "• Pick the top-rated items  \n",
    "\n",
    "**Latent Factor Models**  \n",
    "\n",
    "* User-item rating data is often sparse  \n",
    "* Matrix factorization recommender systems often work well with sparse matrices  \n",
    "* SVD (Singular Value Decomposition) is a very common matrix factorization technique  \n",
    "* SVD is very expensive computationally and needs to be recalculated with new data\n",
    "\n",
    "**Hybrid Recommenders**\n",
    "\n",
    "* All recommenders have strengths and weaknesses   \n",
    "* \"Real world\" recommenders tend to use many models  \n",
    "* Individual models may be used conditionally  \n",
    "* Aggregate \"hybrid\" recommenders used  \n",
    "\n",
    "\n",
    "**Always Validate Recommenders**\n",
    "\n",
    "* We choose recommender models that work well on our data    \n",
    "* We know they work using out of sample validation  \n",
    "* Try many models and test if they work    \n",
    "* Create hybrid models and test if they work \n",
    "* Create conditional models and test if they work \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
